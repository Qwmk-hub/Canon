# 핸디형 비전을 활용한 복합기 조작부 상태 감지 시스템 개발

---

## 프로젝트 개요

본 프로젝트는 복합기 조작부의 상태를 **핸디형 비전 카메라**로 실시간 촬영하여  
이미지 기반으로 조작 상태를 자동 감지하고 분류하는 **AI 모델** 및 **사용자 인터페이스(UI)**를 개발하는 것을 목표로 한다.

---

### 데이터 구성
- **추출 기준**: video를 10프레임 단위로 이미지 추출
- **클래스 구성**:  
  - `pass`: 16,000장  
  - `target1`: 255장  
  - `target2`: 1,428장  
  - `target3`: 110장  
  - `target4`: 137장
- **검증 데이터**: video4에서 1 cycle 이미지 사용
- **데이터 불균형 해결**: 증강 기법 활용

### 모델 구조 및 학습 설정
- **사용 모델**: `EfficientNet`, `ConvNext` (실시간 처리에 적합한 CNN 계열)
- **학습 설정**:
  - Loss Function: `CrossEntropyLoss`
  - Optimizer: `Adam`
  - Learning Rate: `1e-4`
  - Accuracy가 1.0 도달 시까지 학습 반복

### 주요 아이디어
> **현장에서 새로운 타겟이 추가될 때, 개발자의 개입 없이도 작동 가능한 시스템을 구축할 수 있는가?**

→ 해결책: 기존 5개의 class에서 **6개의 class**로 확장 가능하도록 모델 설계

---

## 모델 설계 및 학습 결과

### 데이터 확장 및 증강
- 기존 video1~3에서 새로운 클래스를 추출하여 "new class" 추가
- 기존 증강 기법 재사용:
  - `Gaussian Noise`, `Motion Blur`, `Brightness`, `Rotation`, `Hazy Effect`, `Sharpening`, `Affine`

###  학습 과정
1. 기존 "pass" 데이터에서 new class 이미지 제거
2. 영상(video1~3)에서 new class 수집 및 증강
3. 총 6개 클래스 구성
4. Train/Validation 세트로 분리
5. 최종 모델 학습

### 학습 결과
- Epoch 15 도달 시 정확도 1.0
- 에폭당 30초, 총 학습시간 약 450초
- 최종 모델 저장: `best_model.pth`, `final_model.pth`

### 테스트 결과
- video4를 통해 총 2,012장의 이미지 테스트
- 예측 시간 대부분 0.004초 이내로 빠름
- 높은 정확도로 pass, target1~4, new class 구분 성공

---

## UI 구현

### 기능 요약

| 기능 | 설명 |
|------|------|
| **전이 학습** | 기존 ConvNext에 새로운 클래스 추가 학습 지원 |
| **데이터 증강** | UI에서 증강 기법 선택 가능, 자동 생성 |
| **클래스 추가** | Class 추가/삭제 버튼으로 유동적 클래스 변경 가능 |
| **학습 설정** | 하이퍼파라미터 설정 가능, Train 버튼으로 실행 |
| **학습 방법** | Train/Test/Validation 데이터 분리, Fold 기반 학습 |
| **설정 인터페이스** | Dropdown 및 설정 변경 기능 제공 |

###  장/단점 분석

- 장점:
  - 직관적인 UI
  - 빠른 학습/재학습 가능
  - 개발자 개입 최소화
- 단점:
  - 고성능 컴퓨팅 자원 필요
  - 딥러닝 환경에 따라 학습 실행 어려움 가능성

---

## 모델 적용 영상

> 모델이 실제 조작부 영상을 기반으로 작동하는 과정을 시연한 영상 첨부 (별도 파일/링크 제공 예정)

---

## 결론

본 프로젝트는 **복합기 조작 상태 자동 감지**를 위한  
효율적인 딥러닝 모델과 사용자 친화적 UI를 결합하여,  
**현장 확장성**, **데이터 유연성**, **학습 자동화**를 동시에 충족하는 시스템을 구현하였다.
